# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12LiPHU9Adq-OQ4skGvMRZzRLCUEiZsfG
"""

import tensorflow as tf
import keras
import numpy as np
import matplotlib.pyplot as plt

class CustomCallback(keras.callbacks.Callback):
      def on_train_begin(self, logs={}):
        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []}
        self.n = 0
      def on_epoch_end(self, epoch, logs=None):
            keys = list(logs.keys())
            p = logs.get('accuracy')
            print(" End epoch {} of training; got log keys: {} and p is {}".format(epoch, keys, p))
            self.history['val_acc'].append(logs.get('val_accuracy'))
            if epoch>=1:
              if abs(self.history['val_acc'][epoch] - self.history['val_acc'][epoch-1]) < 0.001:
                self.n += 1
              if self.n >= 3:
                self.model.stop_training = True
      def on_train_end(self, logs={}):
           self.epo = len(self.history['val_acc'])
        

callbacks1 = CustomCallback()

class LeNet:
  def __init__(self, number_classes, datasource, data, batchsize):
    #self.optimize = optimizer
    #self.lr_sch = lr_schedule
    #self.inp_size = input_size
    self.number_classes = number_classes
    self.source = datasource
    self.data = data
    self.batch = batchsize
  
  def build_model(self):
      if self.source == 'mnist':
        # add zero padding to images
        self.data['train']['input'] = np.pad(self.data['train']['input'], ((0,0),(2,2),(2,2),(0,0)), 'constant')
        self.data['test']['input'] = np.pad(self.data['test']['input'], ((0,0),(2,2),(2,2),(0,0)), 'constant')
        self.data['val']['input'] = np.pad(self.data['val']['input'], ((0,0),(2,2),(2,2),(0,0)), 'constant')
        shape = (32, 32, 1) 
      elif self.source == 'cifar':
        shape = (32, 32, 3)
      inp = tf.keras.layers.Input(shape = shape)
      b = tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='tanh', padding="VALID", strides=1)(inp)
      b = tf.keras.layers.AveragePooling2D(pool_size=2)(b)
      b = tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation='tanh', padding="VALID", strides=1)(b)
      b = tf.keras.layers.AveragePooling2D(pool_size=2)(b)
      b = tf.keras.layers.Flatten()(b)
      b = tf.keras.layers.Dense(units=120, activation='sigmoid')(b)
      b = tf.keras.layers.Dense(units=84, activation='sigmoid')(b)
      out = tf.keras.layers.Dense(units=self.number_classes, activation='softmax')(b)
      model = tf.keras.models.Model(inp, out)
      return model

  def compile_model(self):
      self.model = self.build_model()
      self.model.compile(optimizer = tf.keras.optimizers.Adam(), metrics=['accuracy'], loss='sparse_categorical_crossentropy')
      self.hist = self.model.fit(x = self.data['train']['input'], y = self.data['train']['label'], batch_size= self.batch, epochs=30, callbacks=[callbacks1], 
                       validation_data=[self.data['val']['input'], self.data['val']['label']])
  def accuracy(self):
      self.loss_accuracy = self.model.evaluate(self.data['test']['input'], self.data['test']['label'], verbose=0)
      self.ep = callbacks1.epo
      return self.loss_accuracy[1], self.ep

  def plot_hist(self):
      fig, (ax, ax1) = plt.subplots(2,1,figsize=(10,10))
      ax.plot(self.hist.history['loss'])
      ax.plot(self.hist.history['val_loss'])
      title = 'Loss function' + ' (' + 'LeNet' + '_' + self.source.upper() +')' 
      ax.set_title(title)
      ax.set_xlabel('Epoch')
      ax.set_ylabel('Loss')
      ax.grid()
      ax.legend(['Training data', 'Test data'])

      ax1.plot(self.hist.history['accuracy'])
      ax1.plot(self.hist.history['val_accuracy'])
      title = 'Model accuracy' + ' (' + 'LeNet' + '_' + self.source.upper() + ')'
      ax1.set_title(title)
      ax1.set_xlabel('Epoch')
      ax1.set_ylabel('Accuracy')
      ax1.legend(['training data','test data'])

callbacks2 = CustomCallback()

class AlexNet:
  def __init__(self, number_classes, datasource, data, batchsize):
    #self.optimize = optimizer
    #self.lr_sch = lr_schedule
    #self.inp_size = input_size
    self.number_classes = number_classes
    self.source = datasource
    self.data = data
    self.batch = batchsize
  def build_model(self): 
      if self.source == 'mnist':
        self.data['train']['input'] = np.pad(self.data['train']['input'], ((0,0),(2,2),(2,2),(0,0)), 'constant')
        self.data['test']['input'] = np.pad(self.data['test']['input'], ((0,0),(2,2),(2,2),(0,0)), 'constant')
        self.data['val']['input'] = np.pad(self.data['val']['input'], ((0,0),(2,2),(2,2),(0,0)), 'constant')
        shape = (32, 32, 1)  
      elif self.source == 'cifar':
        shape = (32, 32, 3)
      inp = tf.keras.layers.Input(shape=shape)
      b = tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding='valid', activation='relu')(inp)
      b = tf.keras.layers.AveragePooling2D(pool_size=(3,3), strides=2, padding='same')(b)
      b = tf.keras.layers.Conv2D(filters=384, kernel_size= 3, padding='same', strides=1, activation='relu')(b)
      b = tf.keras.layers.Conv2D(filters=384, kernel_size=3, padding='same', strides=1)(b)
      b = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, activation='relu', padding='same')(b)
      b = tf.keras.layers.AveragePooling2D(pool_size=3, strides=2)(b)
      b = tf.keras.layers.Flatten()(b)
      b = tf.keras.layers.Dense(units=4096, activation='relu')(b)
      b = tf.keras.layers.Dense(units=4096, activation='relu')(b)
      out = tf.keras.layers.Dense(units=10, activation='softmax')(b)

      model = tf.keras.models.Model(inp, out)
      return model

  def compile_model(self):
      self.model = self.build_model()
      self.model.compile(optimizer = tf.keras.optimizers.Adam(), metrics=['accuracy'], loss='sparse_categorical_crossentropy')
      self.hist = self.model.fit(x = self.data['train']['input'], y = self.data['train']['label'], batch_size= self.batch, epochs=30, callbacks=[callbacks2], 
                       validation_data=[self.data['val']['input'], self.data['val']['label']])

  def accuracy(self):
      self.loss_accuracy = self.model.evaluate(self.data['test']['input'], self.data['test']['label'], verbose=0)
      self.ep = callbacks2.epo
      return self.loss_accuracy[1], self.ep

  def plot_hist(self):
      fig, (ax, ax1) = plt.subplots(2,1,figsize=(10,10))
      ax.plot(self.hist.history['loss'])
      ax.plot(self.hist.history['val_loss'])
      title = 'Loss function' + ' (' + 'AlexNet' + '_' + self.source.upper() +')' 
      ax.set_title(title)
      ax.set_xlabel('Epoch')
      ax.set_ylabel('Loss')
      ax.grid()
      ax.legend(['Training data', 'Test data'])

      ax1.plot(self.hist.history['accuracy'])
      ax1.plot(self.hist.history['val_accuracy'])
      title = 'Model accuracy' + ' (' + 'AlexNet' + '_' + self.source.upper() + ')'
      ax1.set_title(title)
      ax1.set_xlabel('Epoch')
      ax1.set_ylabel('Accuracy')
      ax1.legend(['training data','test data'])